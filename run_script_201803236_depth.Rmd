---
title: Tuning the GBM tree depth
author: "Sidharth Gupta"
date: \today
output:
   html_document:
    css: css/code.css
    highlight: default
    toc: true
    toc_depth: 6
    citation_package:
    keep_tex: false
    fig_caption: true
    latex_engine: pdflatex
    keep_md: yes
runtime: shiny
fontsize: 11pt
geometry: margin=1ine
header-includes:
- \usepackage{indentfirst}
- \usepackage{graphicx}
- \usepackage{geometry}
- \usepackage{subfigure}
- \usepackage{amsmath}
- \usepackage{listings}
- \usepackage{tikz}
- \usetikzlibrary{matrix}
---

```{r setup, tidy = TRUE, message = FALSE, warning = FALSE, error = FALSE, results='hide'}
library(dplyr)
library(magrittr)
library(gridExtra)
library(ggplot2)
library(h2o)
library(gridExtra)
library(futile.logger)
source("h2o/source-for-rmd.R")
H2O.PORT=54321
H2O.HOST="localhost" # TODO: Remote?
SEED=123456
h2o.init()
h2o.init(ip=H2O.HOST, port=H2O.PORT)
```

## Task: Now that the learning rate is sort of decided we want to tune the tree depth.

```{r data, cache=TRUE, message=FALSE, error=FALSE, warning=FALSE, echo=FALSE, results='hide'}
# The function for reading the data does a few things by default. It makes categorical  variables where ever necessar, and log transforms the large variables like for taxes etc. See documentation of that function.
list[tr.baseline, pr.baseline, vtreatIdentityFn, tplan.NULL, t.dates] =  prepareDataWrapper.h2o.gbm.baseline(dates.to.numeric=T, keep.dates=T)

independent.vars = colnames(tr.baseline %>% select(-logerror))

independent.vars = colnames(tr.baseline %>% select(-logerror))


list[XYTrain.h2o, XYTest.h2o] = h2o.splitFrame(as.h2o(tr.baseline), ratios = 0.85, destination_frames=c("XYTrain.h2o", "XYTest.h2o"), seed=SEED)
```

### Add the GBM depth parameters 
We reuse the code from last notebook. The only part I will change is the depth parameter:
```{r hyperparams_search_crit, echo=FALSE}
hyper_params_search_crit.depth.tune = function(
            ntrees_opts = 10000, # early stopping will stop the earlier.
            max_depth_opts = 10,  # Start with 6 for now
            min_rows_opts = 5,   # How many leaf nodes to average over, doing the log of # rows would be ok
            learn_rate_opts = 0.01, # Chosen from last notebook on learning rate
            learn_rate_annealing = 0.995, # Chosen from last notebook
            sample_rate_opts = 0.9,
            col_sample_rate_opts = 0.9,
            col_sample_rate_per_tree_opts = 0.9,
            strategy="RandomDiscrete") {

    list[search_criteria, hyper_params] =  hyper_params_search_crit(
            ntrees_opts = ntrees_opts,
            max_depth_opts = max_depth_opts,
            min_rows_opts = min_rows_opts,
            learn_rate_opts = learn_rate_opts, # from the previous
            learn_rate_annealing = learn_rate_annealing,
            sample_rate_opts = sample_rate_opts,
            col_sample_rate_opts = col_sample_rate_opts,
            col_sample_rate_per_tree_opts = col_sample_rate_per_tree_opts,
            strategy=strategy) 
    return(list(search_criteria, hyper_params))
}

```

```{r hyperparams, cache=TRUE, results='hide'}
set.seed(SEED) 
NGrids = 1 # Just train the model once
max_depth_opts = seq(5,15)
learn_rate_opts = c(0.01, 0.02)
list[search_criteria.gbm.depth, hyper_params.gbm.depth] = hyper_params_search_crit.depth.tune(
              max_depth_opts=max_depth_opts,
              learn_rate_annealing=0.995,
              learn_rate_opts=learn_rate_opts)
```

```{r training, eval=FALSE, echo=FALSE, results='hide'}
fit.depth.tune = baseline.fit(
       hp=hyper_params.gbm.depth,
       sc=search_criteria.gbm.depth,
       grid_id = "depth_search",
       seed = SEED, 
       nfolds=5)
```


So how do the learning curves look for each fit? First extract the score histories of each of the models
```{r modellist, results='hide', eval=F, echo=F}
# A list of lists of all the trained models
models.lst = modelListFromGrids("depth_search_", NGrids)

# Sanity check: each column is one grid. There are NGrids grids
# and each grid has length(target_learning_rates) models
assertthat::assert_that(nrow(models.lst) == length(max_depth_opts)*length(learn_rate_opts))
assertthat::assert_that(ncol(models.lst) == NGrids)

# For each model list in models.lst extract the score histories
g11 = h2o.getGrid("depth_search_1", sort_by="RMSE") #cache the one grid for offline access
score.history.lst = sapply(X = 1:NGrids,
                           function(x) getRMSEScoreHistory(models.lst[, x]))

```

Retriving the score history for the entire grid in a data frame.

```{r scoreHistory, results='hide', echo=F, eval=F}
# The axes limits

score.history.combined =
    Reduce(function(x, init) {
               rbind(x, init)
          }, Map(function(i) {
                score.history.lst[, i] %>%
                    data.frame %>%
                    mutate(grid_id=i)
            }, 1:NGrids)
    )
```


Plot the score history for each learning rate vs depth!
```{r plots, fig.width=10, fig.height=15, eval=F, echo=F}

# I initially tried working with a lits of plots instead of plotting the data directly. Had trouble sizing the plots properly. So backed off and instead used a combined version of score history with vanialla ggplot.
min_y_lim = score.history.combined %>% pull(rmse) %>% min
max_y_lim = score.history.combined %>% pull(rmse) %>% max
score.history.combined %<>% group_by(depth, lr) %>%  mutate(min_rmse_by_depth =  min(rmse)) %>% ungroup

ggplot(score.history.combined) +
    facet_wrap(~ depth, ncol=2) +
        geom_line(aes(y=rmse, x=ntrees, 
                      color=as.factor(lr)),
                  position=position_dodge(1)) +
        geom_hline(aes(yintercept = min_rmse_by_depth, color=as.factor(lr)), size=0.6, alpha=0.5) +
        ylim(c(min_y_lim, max_y_lim)) +
        theme(
          legend.position="bottom",
          strip.background = element_blank(),
          strip.text.x = element_text(size=6)
        ) + guides(col=guide_legend(title="learning_rate", position="bottom"))

```
So clearly the learning rate of 0.02 is beating out 0.01 and the optimum depth of it seems to be 10-14
as you can see by the horizontal lines. 6 out of the top 10 learning rates are 0.02:

```{r}

g11@summary_table %>% select(learn_rate, max_depth, rmse) %>% data.frame
```

These values of depth(8-14) and learning_rate(0.01,0.02) will be the parameters that shall be used in the grid search. One thing I noticed was that upon running this script with different splits results in slightly different optimal treedepths.

```
