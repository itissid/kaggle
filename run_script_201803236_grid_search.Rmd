---
title: Tuning rest of the GBM parameters
author: "Sidharth Gupta"
date: \today
output:
   html_document:
    toc: true
    toc_depth: 6
    css: css/code.css
    highlight: default
    citation_package:
    keep_tex: false
    fig_caption: true
    latex_engine: pdflatex
    keep_md: yes
   md_document:
    toc: true
    toc_depth: 6
    variant: markdown_github

runtime: shiny
fontsize: 11pt
geometry: margin=1ine
header-includes:
- \usepackage{indentfirst}
- \usepackage{graphicx}
- \usepackage{geometry}
- \usepackage{subfigure}
- \usepackage{amsmath}
- \usepackage{listings}
- \usepackage{tikz}
- \usetikzlibrary{matrix}
---

#### The task: To tune the parameters of the grid and make submission to Kaggle
##### Important Design Considerations I had to make
- Instead of cross validating and searching a parameter space of size 10's of thousands, I first narrow the search space using Random Grid search on non crossvalidated GBM models.
- Crossvalidation is used a bit differently. The parameter space although narrowed is still large so I take the top `K` models and crossvalidate *each* of them to understand the variation of the RMSE estimates. As it turns out these crossvalidated GBM models also do better on the holdout data set. Realize that crossvalidation is used here for bootstrapping the RMSE estimate and we can study the variance of the individual models.
- H2O cannot parallize the initial RandomGrid Search, but it can do so the cross validation. So for tuning the parameter space I use an EC2 cluster to speed things up by training `NGrid` grids and flatten them to get all the models.

##### AWS Cluster details
You can use your local machine to execute the code here. But an AWS cluster may be useful if you don't have computing resources, or just want to speed things up. [Here](https://github.com/itissid/kaggle/blob/h2o-gradient-boost-baseline/images/h2o-cluster.png) is the simple set of steps I follow when using the AWS cluster for doing my h2o work. Some things to keep in mind while you do this are:

- Get a free AWS account and create a secret key. Use an existing AMI image with h2o and R loaded on it and learn how to update the image.
- I am using an AWS cluster on which I start an h2o cluster. I use [this](https://github.com/h2oai/h2o-3/tree/master/ec2) guide to launch the AWS cluster and then start the h2o instances on it.
- Once you have started the cluster grab one of the DNS names and stick it into the CLUSTER.NODE.IP variable.
- You may need to open ports on your EC2 machines, see Security Groups in AWS.
- You can look at the h2o cluster status which is pretty helpful to see the CPU and memory as well. as well as running jobs. [This](https://github.com/itissid/kaggle/blob/h2o-gradient-boost-baseline/images/h2o-cluster.png) is how it looks like once your cluster is running.

#### Setup
```{r setup, tidy = TRUE, message = FALSE, warning = FALSE, error = FALSE}
library(dplyr)
library(magrittr)
library(ggplot2)
library(h2o)
library(kableExtra)
source("utils/AmazonEC2.R")
source("h2o/source-for-rmd.R")
SEED=123456
SNOW.PORT=12345 # This port should also be opened in the AWS Security group with which the instances were created.
CLUSTER.NODE.IP='localhost' # This would be the IP ADDESS of the h2o cluster (I regularly use ec2 DNS name here)
CORES.PER.MACHINE=2 # Used for the SNOW cluster.
REMOTE.RESULTS.DIR="/home/ubuntu/workspace/kaggle/results/2018_03_04_gbm" # h2o.saveModel or saveGrid will save data to the machine where the cluster connection is initiated to(for this notebook its the CLUSTER.NODE>UP)
LOCAL.RESULTS.DIR=paste0(getwd(), "/results/2018_03_04_gbm")
if(!dir.exists(LOCAL.RESULTS.DIR)) {
    dir.create(LOCAL.RESULTS.DIR, recursive=TRUE)
}
DATE.FEATURE=TRUE # If I use data with date as a feature I use this
```

```{r, echo=FALSE}
# KNITR CONTROL OPTIONS:
LOAD.DATA=FALSE # Once you run the NB and loaded the data you may want to make this false
EVALUATE.DISK.MODELS=FALSE # Once you run the NB the models will be loaded to disk, you might want to kill this option then
```



##### Setting up the data
The only change here is I am using dates as features and using a separate test and validation sets.
```{r data, cache=TRUE, message=FALSE, error=FALSE, warning=FALSE, echo=TRUE, results='hide', eval=LOAD.DATA==TRUE}
h2o.connect(ip=CLUSTER.NODE.IP)
# The function for reading the data does a few things by default. It makes categorical  variables where ever necessar, and log transforms the large variables like for taxes etc. See documentation of that function.
list[tr.baseline, pr.baseline, vtreatIdentityFn, tplan.NULL, t.dates] =  prepareDataWrapper.h2o.gbm.baseline(dates.to.numeric=T, keep.dates=DATE.FEATURE)

independent.vars = colnames(tr.baseline %>% select(-logerror))

list[XYTrain.h2o, XYValidation.h2o, XYTest.h2o] = h2o.splitFrame(as.h2o(tr.baseline), ratios = c(0.70, 0.15), destination_frames=c("XYTrain", "XYValidation", "XYTest"), seed=SEED)

assertthat::assert_that("date" %in% independent.vars)
```

First lets initialize the hyper parameters as follows
```{r hyperparams_search_crit}

list[search_criteria.gbm, hyper_params.gbm] =
    hyper_params_search_crit(
          max_depth_opts=seq(8, 14, 1),
          learn_rate_opts=c(0.01, 0.02),
          ## search a large space of row sampling rates per tree
          sample_rate = seq(0.4,1,0.2),

          ## search a large space of column sampling rates per split
          col_sample_rate_opts = seq(0.4,1,0.2),

          ## search a large space of column sampling rates per tree
          col_sample_rate_per_tree_opts = seq(0.4,1,0.2),

          ## search a large space of how column sampling per split should change as a function of the depth of the split
          col_sample_rate_change_per_level_opts = seq(0.9,1.1,0.1),

          ## search a large space of the number of min rows in a terminal node
          min_rows_opts = c(5, 10, 15, 20),

          ## search a large space of the number of bins for split-finding for continuous and integer columns
          nbins_opts = 2^seq(4,10,1),

          ## search a large space of the number of bins for split-finding for categorical columns
          nbins_cats_opts = 2^seq(4,10,1),

          ## search a few minimum required relative error improvement thresholds for a split to happen
          min_split_improvement_opts = c(1e-8, 1e-6, 1e-4),

          ## try all histogram types (QuantilesGlobal and RoundRobin are good for numeric columns with outliers)
          histogram_type_opts = c("UniformAdaptive","QuantilesGlobal","RoundRobin"),
	  strategy="RandomDiscrete"
    )

```

#### Tuning the grid(without CrossValidation)

Here is the wrapper written to do the model fit.
```{r}

baseline.fit = function(hp, sc, independent.vars, grid_id, seed=SEED) {
        h2o.connect()
        m1 = paste0(names(hp), hp, collapse='\n')
        cat(paste0("Grid executing with params: \n", m1), file=stdout())
        h2o.grid.helper(
                        h2o.getFrame("XYTrain"),
                        h2o.getFrame("XYValidation"),
                        independentCols=independent.vars,
                        hyper_params = hp,
                        search_criteria = sc,
                        algorithm="gbm",
                        grid_id=grid_id,
                        seed = seed)
        cat("Done", file=stdout())
}
```


Now lets train a few models. We train a few models a few times to compare learning rates
```{r training}
sc=search_criteria.gbm
hp=hyper_params.gbm
NGrids = length(hp$max_depth)*length(hp$learn_rate)

progress <- function() cat('.')
# Training over all the parameter space even with RandomDiscrete and a few 100 trees can take a long time. So I chose to parallelize over the two most important parameters the depth and the learning rate. This produces a much larger number of models per depth-learningrate combination to chose from.
train.grid.wrapper = function(hp, sc) {
    foreach::foreach(
        nx = itertools::product(
            max_depth = hp$max_depth,
            learn_rate = hp$learn_rate
            ),
        grid_id = iter(1:NGrids),
        .packages=c("h2o", "dplyr", "magrittr", "futile.logger"),
        .export=c("baseline.fit", "h2o.grid.helper", "independent.vars"),
        .options.snow=list(progress=progress)) %dopar% {
                max_depth = nx$max_depth
                learn_rate = nx$learn_rate
                hp_t = hp
                hp_t$max_depth = max_depth
                hp_t$learn_rate = learn_rate

                return(baseline.fit(
                       hp=hp_t,
                       sc=sc,
                       grid_id=grid_id,
		       seed=SEED))
        }
}

pergrid.models = Reduce('*', Map(function(x, init) length(x), hp) )/NGrids
flog.info(paste0( "you will have to do parallel training of ", NGrids , " grids with ", pergrid.models, " models each."))

```

##### An optional Parallel Cluster
As you can see training so many grids can take a long time. Instead we will use a sample them using RandomDiscrete. It is better to spawn use a cluster to do the training of all these grids.
Initialize the AWS h2o cluster using snow
```{r snowcluster, eval=FALSE}

instances = getExistingInstancesPublicDNS()
print(instances)
live_instances = instances
# The snow cluster is used if I want to execute the grid on multiple machines. A regular cluster is used if on local.

spec = clusterSpecFromInstances(live_instances)
clus = registerCluster(localOnly=FALSE, port=SNOW.PORT, instances.opts=spec, CORES.PER.MACHINE)
```

Start the training
```{r, eval=FALSE}
models = train.grid.wrapper(hp, search_criteria.gbm)
```

##### Saving the models for later
The grids above can take a long time to complete. But with `RandomDiscrete` once a few 100 models have been trained we can stop the cluster and crack on with the analysis. Lets save the grids to disk and work offline with them.  You might want to source the code above and the constants to if you do. One issue with an h2o cluster is when interactive mode a failure on one node can cause models to dissapear, so save the models using the rsyn command in a while loop with a sleep.
```{r, eval=FALSE}
h2o.connect(ip=CLUSTER.NODE.IP)
models = h2o.gridSaver(1:NGrids, results.dir=REMOTE.RESULTS.DIR)
```

The above command saves the models to a remote machine. Copy the results to the local disk to do some analysis on them. Assuming you have access to the cluster you can execute the rsync command:
```{r, eval=FALSE}
cwd = getwd()
rsync.command=paste0('mkdir -p results/grid; rsync -Pavz -e "ssh -i $(ls ~/.ssh/sid-aws-key.pem)"  ubuntu@', CLUSTER.NODE.IP ,':', REMOTE.RESULTS.DIR,' ', LOCAL.RESULTS.DIR)
print(rsync.command)
system(rsync.command)
```

...............................................................

...... WAIT SOME TIME TO GET ENOUGH MODELS BEFORE CONTINUING.....

...............................................................

##### Analysis and Tuning

If you made it here, you might have had to deal with a possible crash of your h2o cluster or shutting it down to avoid a massive EC2 bill. In this case you can reload all those saved models from disk:
```{r loadGridModels, eval=FALSE, results='hide'}
models = h2o.gridLoader(1:NGrids, results.dir=LOCAL.RESULTS.DIR)
all_summaries = model_summaries_from_loaded_models(unlist(models)) %>% arrange(validation_rmse)
sorted.summaries = all_summaries %>% arrange(validation_rmse)
```

Lets look at the top 40 models in the grid. The following is a sample of 106 models which is small as compared to the entire grid. A larger sample would be better. But there are still some useful observations that come out:

```{r, eval=FALSE}
all_summaries
```

```{r, eval=TRUE, echo=FALSE}
kable(read.table(text= "nbins_cats learn_rate min_rows sample_rate col_sample_rate  col_sample_rate_per_tree   histogram_type     number_of_trees number_of_internal_trees  mean_depth  validation_rmse train_rmse
    64       0.02       20         1.0             0.6                       0.6       RoundRobin                    200                      200    12.00000        0.1576870  0.1513393
    64       0.02       20         0.8             0.8                       0.6       RoundRobin                    205                      205    12.00000        0.1577111  0.1525869
    64       0.02       20         0.6             0.8                       0.4       QuantilesGlobal               189                      189    14.00000        0.1577426  0.1531752
    64       0.01       10         0.6             0.4                       0.6       RoundRobin                    267                      267    14.00000        0.1577857  0.1522151
    16       0.01       20         0.6             0.6                       1.0       RoundRobin                    238                      238    13.00000        0.1578537  0.1567330
   128       0.02       20         1.0             0.6                       0.8       RoundRobin                    103                      103    14.00000        0.1578568  0.1523961
    32       0.02       15         0.8             0.8                       0.6       QuantilesGlobal               146                      146    12.00000        0.1578600  0.1515632
    32       0.02       20         0.8             0.6                       0.8       QuantilesGlobal               144                      144    10.00000        0.1578612  0.1554228
    32       0.02       20         0.6             1.0                       0.8       RoundRobin                     97                       97    14.00000        0.1578645  0.1556081
    32       0.02       10         0.8             0.8                       0.4       QuantilesGlobal               152                      152    11.00000        0.1578730  0.1513839
    64       0.01       20         0.8             0.4                       1.0       RoundRobin                    275                      275    11.00000        0.1578783  0.1563598
    32       0.01       20         0.8             0.4                       0.8       UniformAdaptive               258                      258    13.00000        0.1578937  0.1559698
    32       0.02       20         1.0             0.4                       0.6       QuantilesGlobal               135                      135    12.00000        0.1579010  0.1520331
    64       0.01       20         0.6             1.0                       0.8       QuantilesGlobal               225                      225    12.00000        0.1579131  0.1558366
   128       0.01       15         0.6             0.4                       0.8       RoundRobin                    241                      241    14.00000        0.1579200  0.1525593
    64       0.02       15         0.4             1.0                       0.4       QuantilesGlobal               158                      158    10.00000        0.1579432  0.1563515
    16       0.02       15         0.8             1.0                       0.6       QuantilesGlobal               113                      113    11.97345        0.1579473  0.1531330
    64       0.02       10         0.8             1.0                       0.6       UniformAdaptive               120                      120    11.00000        0.1579483  0.1539735
    32       0.01       10         0.6             0.8                       0.8       RoundRobin                    211                      211    13.00000        0.1579587  0.1549085
    64       0.02        5         1.0             0.6                       0.4       UniformAdaptive               182                      182     9.00000        0.1579593  0.1524554", header=TRUE))
```

###### Lessons
- nbins_cats of 16-64 range is enough. The top 20 values lean more towards 32 - 64
- the depth of the top trees is between 12-14.
- sample rate is happy being between 0.6-0.8. Although there are two values in there that are 1.0 in top 20
- col_sample_rate for top models is between 0.4-0.8 for the most part
- The QuantilesGlobal and RoundRobin domniate the top 20. We can drop Uniform Adaptive

###### Narrowing the grid
So we can narrow the tuning grid with the following changes

```{r}

hp=hyper_params.gbm
hp$col_sample_rate = seq(0.4, 0.8, 0.1)
hp$max_depth= seq(10, 14, 1)
hp$sample_rate = seq(0.4, 0.8, 0.1)
hp$nbins_cats= 2^seq(4,6,1)
hp$histogram_type= c("QuantilesGlobal", "RoundRobin")
#
NGrids = length(hp$max_depth)*length(hp$learn_rate)
pergrid.models = Reduce('*', Map(function(x, init) length(x), hp) )/NGrids
flog.info(paste0( "After narrowing grid search, parallel training with ", NGrids , " grids with ", pergrid.models, " models each."))
```

This is still quite a large number of models, but random sampling in this space will give us a more accurate idea of what models do best.
```{r trainNewerModels, eval=FALSE}
models = train.grid.wrapper(hp, search_criteria.gbm)
```

Again we save the models
```{r saveNewGrid, eval=FALSE}
h2o.connect(ip=CLUSTER.NODE.IP)
models = h2o.gridSaver(1:NGrids, results.dir=REMOTE.RESULTS.DIR)
```

I ran the grid search again with the new hyper parameters and here are the summaries
```{r loadNewGridModels, results='hide', eval=EVALUATE.DISK.MODELS==TRUE}
models = h2o.gridLoader(1:NGrids, results.dir=LOCAL.RESULTS.DIR)
all_summaries = model_summaries_from_loaded_models(unlist(models)) %>% arrange(validation_rmse)
sorted.summaries = all_summaries %>% arrange(validation_rmse)
topk.ids = sorted.summaries %>% pull(model_id) %>% as.character
```

```{r}
flog.info(paste0("Loaded ", length(models) , " grid models from memory"))
```

```{r}
ds = head(all_summaries %>% select(-model_id, -nbins, -nbins_top_level, -ntrees, -histogram_type,  -min_depth, -max_depth), 20)
kable(ds)
```

##### Summary
- We see here that the sample_rate and col_sample_rate are same as last time
- the top depths that dominate seem to be 13-14
- the approximate train time seems to be within bounds so early stopping is probably due to the # stopping rounds and number of trees


#### CrossValidated GBMs and Simple GBMs
I also looked at how stable these performances were using cross validation on the top K models. Cross validation will produce models with not only smaller error but it will also tell us the variance of the RMSE estimate.

```{r}
# Helper to train for cross validation
retrainWrapper.cv <- function(gbm, train.df, validation.df) {
    return(do.call(h2o.gbm,
        ## update pasameters in place
        {
          p <- gbm@parameters
          p$model_id = NULL          ## do not overwrite the original grid model
          p$training_frame = train.df      ## use the full dataset
          p$validation_frame = validation.df  ## no validation frame
          p$nfolds = 5               ## cross-validation
	  p$seed=SEED
          p
     }))
}
```
##### A local cluster for CrossValidation
Since h2o can do cross validation parallely we can don't need a large cluster.
```{r trainCVGbMs, eval=FALSE}

# The local cluster for doing the parallel computation
cls = registerCluster(max.cores.per.machine = 6)

# Create a list of crossvalidated GBM model, indexed by the topk model id's. 20 top models are selected
cvgbms = foreach::foreach (
       i = iter(1:20), .packages=("h2o"),
       .final = function(x) setNames(x, topk.ids[1:20])) %dopar% {
    h2o.connect()
    gbm <- h2o.getModel(topk.ids[[i]])
    cvmodel = retrainWrapper.cv(
          gbm,
          train.df = h2o.rbind(XYTrain.h2o, XYValidation.h2o),
          validation.df = XYTest.h2o)
    # We note the modelid of the GBM's so as to associate them with the odel from whcih they were created
    cvmodel@model$origin.model = topk.ids[[i]]
    return(cvmodel)
}
```

Optionally save these models for offline work.
```{r saveGbms, cache=TRUE, eval=FALSE}
gbm.results.dir = file.path(LOCAL.RESULTS.DIR, "cv_models")
dir.create(gbm.results.dir)
for(m in cvgbms) {
    if(!is.null(m)) {
        fp = file.path(gbm.results.dir, m@model$origin.model)
        dir.create(fp)
        h2o.saveModel(m, path=fp) # Don't try and save with saveRDS, it does not work
        h2o.download_pojo(m, path=fp) # Don't try and save with saveRDS, it does not work
        cat(".")
    }
}

```

Optionally load the models if you are resuming work on saved models.
```{r loadTunedGBMs, results='hide', eval=EVALUATE.DISK.MODELS==TRUE}
# Since the models are loaded from disk. the order they are in is not the same as the topk.ids. So we need to take care of that here
gbm.results.dir = file.path(LOCAL.RESULTS.DIR, "cv_models")
if(!dir.exists(gbm.results.dir)) {
    flog.warn(paste0("Dir ", gbm.results.dir, " does not exist"))
    stopifnot(FALSE)
}

cvgbms = list()
for(m.id_file in list.files(gbm.results.dir, recursive=TRUE)) {

    m.id_file.path = file.path(gbm.results.dir, m.id_file)

    if(endsWith(m.id_file.path, ".java") || endsWith(m.id_file.path, ".jar")) { # Saved POJOs
        flog.info(paste("Not loading ", m.id_file.path))
        next
    }
    # Find the slot of the loaded model in the current files
    flog.debug(paste0("Loading ", m.id_file.path))
    origin.model = strsplit(m.id_file, "/")[[1]][[1]]
    m = h2o.loadModel(m.id_file.path)
    cvgbms[[origin.model]] = m
    print('..')

}
flog.info("Loaded ", length(cvgbms), " crossvalidated models")
```

##### Comparing the CrossValidated GBMs and the Simple GBM models

First combine the summaries of the scores from the simple GBM models I trained above and the the cross validated GBMs. 
```{r combineScores, results='hide'}

simplegbm.scores.topk = model_metrics_helper(Map(h2o.getModel,topk.ids[1:20]))

xvalgbm.scores.topk = model_metrics_helper(cvgbms[topk.ids[1:20]]) %>%
                    rename(cv_model_id = model_id) %>%
                    mutate(referrer_model_id = topk.ids[1:20])

 # training_rmse: The model's error on the training data set
 # validation_rmse: The model's error on the validation data set
 # training_rmse_cv: The training error on cross validated data,
 # five_fold_rmse_cv": The combined rmse on holdout set of the crossvalidated data
 # five_fold_rmse_sd_cv": the sd of the K fold CV RMSEs
 # validation_rmse_cv: The error of the cross validated model on the hold out data
combined.scores = dplyr::inner_join(
                        simplegbm.scores.topk,
                        xvalgbm.scores.topk,
                        by=c("model_id"="referrer_model_id"),
                        suffix=c("_orig", "_cv")) %>%
			mutate(x=1:nrow(combined.scores))

```

This plot plots the important RMSE scores of
1. The Non cross validated model on the validation data set, `validation_rmse_orig`
2. The cross validated model on the validation data set, `validation_rmse_cv`
3. The "combined RMSE" of the cross validated model on the entire data set. See [this](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/cross-validation.html) for how this score is calculated.

```{r combinedScoresPlot}

ggplot(combined.scores) +
    geom_line(aes(x=x, y = validation_rmse_cv, color='validation_rmse_cv', group=1)) +
    geom_line(aes(x=x, y = validation_rmse_orig, color='validation_rmse_orig', group=1)) +
    geom_line(aes(x=x, y = five_fold_rmse_cv, color='five_fold_rmse_cv', group=1)) +
    geom_errorbar(aes(x=x,
                  ymin = five_fold_rmse_cv + five_fold_rmse_sd_cv,
                  ymax = five_fold_rmse_cv - five_fold_rmse_sd_cv,
                  color='five_fold_rmse_cv', group=1), alpha=0.5, size=0.5)

```

And here are the results:
```{r combined.scoresTable}
# Best model by validation RMSE on original data set
kable(combined.scores %>% dplyr::arrange(validation_rmse_orig) %>% dplyr::select(model_id, cv_model_id, validation_rmse_orig, validation_rmse_cv, five_fold_rmse_cv) %>% head(5))
# Best model by validation RMSE on crossvalidated data
kable(combined.scores %>% dplyr::arrange(validation_rmse_cv) %>% dplyr::select(model_id, cv_model_id, validation_rmse_orig, validation_rmse_cv, five_fold_rmse_cv) %>% head(5))
# Best model by combined nfold RMSE on entire data(See above note)
kable(combined.scores %>% dplyr::arrange(five_fold_rmse_cv) %>% dplyr::select(model_id, cv_model_id, validation_rmse_orig, validation_rmse_cv, five_fold_rmse_cv) %>%  head(5))
```

##### Takeaways
- The crossvalidation improves the performance on the hold out data set(compare `validation_rmse_orig` and `validation_cv_rmse`)
- The cross-validation rmse are pretty high(ish). Its possible that because of the task of modelling the log error the task the amount of noise in the data set is high. Regardless of the cause of the high variance, [this](https://github.com/h2oai/h2o-3/blob/master/h2o-docs/src/product/cross-validation.rst) link indicates that becasue of it, its hard to fully "trust" the top models.
- Off The top models which did well(see validation_rmse) on holdout data set, only one model appeared consistently in the top 5.

#### Prediction


Here is the code for the predictions for the top models
```{r predictionFuncs, results='hide' }

model.ids = c('10_model_0', '8_model_5', '10_model_16', '8_model_18', '10_model_1') # The indexes of the top models
assertthat::assert_that(all(model.ids %in% topk.ids))
# The dates needed for the levels
predict.dates = c("2016-10-01", "2016-11-01", "2016-12-01")
predict.dates.numeric.codes = which(levels(t.dates) %in% predict.dates)
parcel_ids = as.integer(pr.baseline$id_parcel)

predictAndWriteModels = function(model.ids) {
    # Given a list of model.ids predict and write the models to disk,
    # Use the DATE.FEATURES option to predict with date as a feature
    foreach(
        nx = itertools::product(
             model.id = iter(model.ids)
        ),
        .packages=c("magrittr", "h2o", "dplyr")) %do% { # Was not able to make this a dopar on local as it eats a lot of memory and crashes the R session
            cat(paste0("Prediction for Model: ", nx$model.id, "\n"), stdout())
            #h2o.connect() # In case of parallel
            print(".")
            if(DATE.FEATURE) {
                print("Predicting for date features included")
                ds = Reduce(cbind, Map(function(date.code) {
                    flog.debug("Loading the prediction data set into h2o")
                    if('XPredict' %in% h2o.ls()$key) {
                        X = h2o.getFrame("XPredict")
                        X[, "date"] = date.code
                    } else {
                        X = as.h2o(pr.baseline %>% dplyr::mutate(date=date.code), destination_frame="XPredict")
                    }
                    ds = h2o.predict(h2o.getModel(nx$model.id), X)
                    rm(X)
                    gc()
                    print("..")
                    return(as.data.frame(ds))
                }, predict.dates.numeric.codes))
                print("...")
                ds = cbind(data.frame(parcelid=parcel_ids), ds, ds)
                colnames(ds) = c("parcelid", "201610", "201611" ,"201612", "201710", "201711", "201712")
                print("....")
            } else {
                if('XPredict' %in% h2o.ls()$key) {
                    predictions = h2o.predict(h2o.getModel(nx$model.id), h2o.getFrame('XPredict')) %>%
                    as.data.frame
                } else {
                    predictions = h2o.predict(h2o.getModel(nx$model.id),
                                              as.h2o(pr.baseline, destination_frame="XPredict")) %>%
                                  as.data.frame
                }

                ds = data.frame(parcelid=parcel_ids,
                                "201610"=predictions,
                                "201611"=predictions,
                                "201612"=predictions,
                                "201710"=predictions,
                                "201711"=predictions,
                                "201712"=predictions)
                colnames(ds) = c("parcelid", "201610", "201611" ,"201612", "201710", "201711", "201712")
            }

            print(paste(dim(ds)))
            h2o.rm("XPredict")
            writePredictions(predictions = ds,
                             filename.suffix=paste0(
                                    "cross_validated_",  nx$model.id))
            rm(ds)
            gc()
    }
}
```

I made a number of submissions to kaggle:

    10_model_0  -> # 3rd model by all the scores -> 0.0649718 * Third best
    8_model_5   -> # Top five_fold_rmse, #4 validation_cv_rmse  -> 0.0651885
    10_model_16 -> # Top validation_cv_rmse score  -> 0.0652243
    8_model_18  -> #3 in five_fold_cv_rmse # 5 in validation_rmse -> 0.0649474 ** Second Best
    10_model_1  -> #1 in validation_rmse -> **0.0649084**  The best performing model

```{r cvPredictions, eval=FALSE}
# Predict the crossvalidated GBM's
predictAndWriteModels(Map(function(model.id) cvgbms[[model.id]]@model_id, model.ids))
```

#### Ensemble prediction
Also note that if you retrain the best model on the entire data set it does worst on the final kaggle submission, likely overfitting.
One thing to keep in mind with h2o ensembles is that it requires a few things to be same for the base models, documented [here](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/stacked-ensembles.html)
```{r ensembles, eval=FALSE}
best = c("10_model_1")
nexttwo= c("10_model_0" , "8_model_18" )
top3gbms = c(best, nexttwo)

# According to http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/stacked-ensembles.html I need to
# train the base learners but for some reason I get a NPE on models I had loaded from teh disk, due to [this](https://stackoverflow.com/questions/44751866/h2o-nullpointerexception-error-while-building-ensemble-model-using-deep-learni) so I ll retrain the best models again and ensemble them. which should not take long

df = as.h2o(h2o.rbind(XYTrain.h2o, XYValidation.h2o),
            destination_frame="XYtrain_valid")
cvmodels.top = foreach(
                   gbm = iter(Map(h2o.getModel, top3gbms)),
                   .packages=c("h2o", "dplyr", "magrittr", "futile.logger"),
                   .export=c("baseline.fit", "h2o.grid.helper", "independent.vars")) %dopar% {
    do.call(h2o.gbm,
        {
          p <- gbm@parameters
          p$model_id = NULL          ## do not overwrite the original grid model
          p$training_frame = h2o.getFrame("XYtrain_valid") ## use the full dataset
          p$validation_frame = h2o.getFrame("XYTest")  ## no validation frame
          p$seed=SEED                # We need to keep the same seed for ensembles
          p$nfolds = 5               ## cross-validation
          p
     })
}

ensemble.glm = h2o.stackedEnsemble(
            x = independent.vars,
            y = "logerror",
            training_frame = df,
            validation_frame = XYTest.h2o,
            metalearner_nfolds=5,
            base_models=cvmodels.top)
ensemble.gbm = h2o.stackedEnsemble(
            x = independent.vars,
            y = "logerror",
            training_frame = df,
            validation_frame = XYTest.h2o,
            metalearner_nfolds=5,
            metalearner_algorithm="gbm",
            base_models=cvmodels.top)
```

```{r ensembleMetrics}
# Eval ensemble performance on a test set
ensemble.metrics = model_metrics_helper(list(ensemble.glm, ensemble.gbm)) %>% mutate(model_type=c("GLM", "GBM"))

# Compare to base learner performance on the test set
baselearner.metrics <- model_metrics_helper(cvmodels.top)
kable(ensemble.metrics)
kable(baselearner.metrics)
```

The GLM ensemble seems to give a better cross validation RMSE (`five_fold_rmse`) and  better `validation_rmse` scores. Lets see how the scores are on the kaggle leader board.

```{r predictEnsembles, eval=FALSE}

predictAndWriteModels(Map(function(m) m@model_id, c(ensemble.glm, ensemble.gbm)))
```

The ensemble.glm achieved a score of *0.0654413* and the ensemble.gbm achieved a score of *0.0658811*, which was poorer than the base models and poorer than the best crossvalidated GBM earlier in the notebook which scored `0.0649084`. This was a bit unexpected. One reason might be that the models may not be diverse enough, I found that the ensemble models had high correlation and low RMSE for the validation data set predictions. But its not guaranteed that this is the only reason.[1](https://link.springer.com/content/pdf/10.1023/A:1022859003006.pdf)

Lets see if I can improve on the scores by ensembling a model like random forests and some feature engineering.
