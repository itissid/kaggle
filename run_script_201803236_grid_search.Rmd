---
title: Tuning rest of the GBM parameters
author: "Sidharth Gupta"
date: \today
output:
   html_document:
    css: css/code.css
    highlight: default
    citation_package:
    keep_tex: false
    fig_caption: true
    latex_engine: pdflatex
runtime: shiny
fontsize: 11pt
geometry: margin=1ine
header-includes:
- \usepackage{indentfirst}
- \usepackage{graphicx}
- \usepackage{geometry}
- \usepackage{subfigure}
- \usepackage{amsmath}
- \usepackage{listings}
- \usepackage{tikz}
- \usetikzlibrary{matrix}
---

```{r setup, tidy = TRUE, message = FALSE, warning = FALSE, error = FALSE}
library(dplyr)
library(magrittr)
library(gridExtra)
library(ggplot2)
library(h2o)
library(gridExtra)
source("h2o/source-for-rmd.R")
SEED=123456
```

##### The task: To determine the other parameters of the grid

```{r data, cache=TRUE, message=FALSE, error=FALSE, warning=FALSE, echo=TRUE, results='hide'}
# The function for reading the data does a few things by default. It makes categorical  variables where ever necessar, and log transforms the large variables like for taxes etc. See documentation of that function. 
list[tr.baseline, pr.baseline, vtreatIdentityFn, tplan.NULL] =  prepareDataWrapper.h2o.gbm.baseline()

list[XYTrain.h2o, XYValidation.h2o, XYTest.h2o] = h2o.splitFrame(as.h2o(tr.baseline), ratios = c(0.70, 0.15), destination_frames=c("XYTrain", "XYValidation", "XYTest"))
independent.vars = colnames(tr.baseline %>% select(-logerror))

# XPredict = as.h2o(pr.baseline, destination_frame="XPredict")
```

First lets initialize the hyper parameters as follows
```{r hyperparams_search_crit}

list[search_criteria.gbm, hyper_params.gbm] = 
    hyper_params_search_crit(
          max_depth_opts=seq(8, 14, 1), 
          learn_rate_opts=c(0.01, 0.02),
          ## search a large space of row sampling rates per tree
          sample_rate = seq(0.4,1,0.2),                                             
          
          ## search a large space of column sampling rates per split
          col_sample_rate_opts = seq(0.4,1,0.2),                                         
          
          ## search a large space of column sampling rates per tree
          col_sample_rate_per_tree_opts = seq(0.4,1,0.2),                                
          
          ## search a large space of how column sampling per split should change as a function of the depth of the split
          col_sample_rate_change_per_level_opts = seq(0.9,1.1,0.1),                      
          
          ## search a large space of the number of min rows in a terminal node
          min_rows_opts = c(5, 10, 15, 20),
          
          ## search a large space of the number of bins for split-finding for continuous and integer columns
          nbins_opts = 2^seq(4,10,1),                                                     
          
          ## search a large space of the number of bins for split-finding for categorical columns
          nbins_cats_opts = 2^seq(4,10,1),                                                
          
          ## search a few minimum required relative error improvement thresholds for a split to happen
          min_split_improvement_opts = c(1e-8, 1e-6, 1e-4),                               
          
          ## try all histogram types (QuantilesGlobal and RoundRobin are good for numeric columns with outliers)
          histogram_type_opts = c("UniformAdaptive","QuantilesGlobal","RoundRobin"),
	  strategy="RandomDiscrete"
    )

```


Here is the wrapper written to do the model fit.
```{r}

baseline.fit = function(hp, sc, grid_id, seed=SEED) {
        h2o.grid.helper(
                        XYTrain.h2o, 
                        XYValidation.h2o, 
                        independentCols=independent.vars,
                        hyper_params = hp,
                        search_criteria = sc,
                        algorithm="gbm", 
                        grid_id=grid_id,
                        seed = seed
                        )
}
```

Now lets train a few models. We train a few models a few times to compare learning rates

```{r training, cache=TRUE}
# TODO: Register a parallel backend using snow/doParallel
# Run the for each loop 
# Get Cluster spec from h2o cluster.
# Register parallel backend for that cluster
# train the grids
instances = getExistingInstancesPublicDNS()
print(instances)
live_instances = instances[[2]]
spec = clusterSpecFromInstances(live_instances)

#cl <- parallel::makeCluster(rep(spec, ncores_p));

clus = registerDoParallelWrapper(log.file.prefix="gbm.cluster", max.cores=6, cluster_spec=spec)
# foreach is quite expensive to just have iterators over all the hyper params so we have multiple grids and we iterate only over learning rate and depth
sc=search_criteria.gbm
hp=hyper_params.gbm
grids = foreach::foreach(
		nx = itertools::product(
		    max_depth = hp$max_depth,
		    learn_rate = hp$learn_rate,
		    grid_id = length(hp$max_depth)*length(hp$learn_rate))
		) %do% {
			hp$max_depth = nx$max_depth
			hp$learn_rate = nx$learn_rate 
			return(baseline.fit(
			       hp=hp, 
			       sc=sc, 
			       grid_id =grid_id,
			       seed = as.integer(runif(1, 1e5, 1e6))))
						
		}
```


