---
title: Tuning rest of the GBM parameters
author: "Sidharth Gupta"
date: \today
output:
   html_document:
    css: css/code.css
    highlight: default
    citation_package:
    keep_tex: false
    fig_caption: true
    latex_engine: pdflatex
runtime: shiny
fontsize: 11pt
geometry: margin=1ine
header-includes:
- \usepackage{indentfirst}
- \usepackage{graphicx}
- \usepackage{geometry}
- \usepackage{subfigure}
- \usepackage{amsmath}
- \usepackage{listings}
- \usepackage{tikz}
- \usetikzlibrary{matrix}
---

```{r setup, tidy = TRUE, message = FALSE, warning = FALSE, error = FALSE}
library(dplyr)
library(magrittr)
library(ggplot2)
library(h2o)
source("utils/AmazonEC2.R")
source("h2o/source-for-rmd.R")
SEED=123456
SNOW.PORT=12345 # This port should also be opened in the AWS Security group with which the instances were created.
CLUSTER.NODE.IP='localhost' # This would be the IP ADDESS of the h2o cluster
CORES.PER.MACHINE=2 # For the multi machine AWS cluster
REMOTE.RESULTS.DIR="/home/ubuntu/workspace/kaggle/results/2018_03_04_gbm"
LOCAL.RESULTS.DIR=paste0(getwd(), "/results/2018_03_04_gbm")
if(!dir.exists(LOCAL.RESULTS.DIR)) {
    dir.create(LOCAL.RESULTS.DIR, recursive=TRUE)
}
DATE.FEATURE=TRUE
```


##### The task: To determine the other parameters of the grid
##### Main Points
- Instead of cross validating a parameter space of size 10's of thousands. We narrow it first using Random grid search.
- We choose the top `K`  models from this Random Grid search and instead cross validate each one of them(Like choosing exactly K parameters). I will choose 3 models to make prediction:
    - The lowest average crossvalidated RMSE
    - The model with the lowest cross validated SD
    - The model with the lowest CV score(just for fun)

TODO: Link a page to start the h2o cluster.
Once you have started the cluster grab one of the IP's and stick it into the CLUSTER.NODE.IP variable
```{r data, cache=TRUE, message=FALSE, error=FALSE, warning=FALSE, echo=TRUE, results='hide', eval=FALSE}
h2o.connect(ip=CLUSTER.NODE.IP)
# The function for reading the data does a few things by default. It makes categorical  variables where ever necessar, and log transforms the large variables like for taxes etc. See documentation of that function.
list[tr.baseline, pr.baseline, vtreatIdentityFn, tplan.NULL, t.dates] =  prepareDataWrapper.h2o.gbm.baseline(dates.to.numeric=T, keep.dates=DATE.FEATURE)

list[XYTrain.h2o, XYValidation.h2o, XYTest.h2o] = h2o.splitFrame(as.h2o(tr.baseline), ratios = c(0.70, 0.15), destination_frames=c("XYTrain", "XYValidation", "XYTest"), seed=SEED)
independent.vars = colnames(tr.baseline %>% select(-logerror))

# XPredict = as.h2o(pr.baseline, destination_frame="XPredict")
```

First lets initialize the hyper parameters as follows
```{r hyperparams_search_crit}

list[search_criteria.gbm, hyper_params.gbm] =
    hyper_params_search_crit(
          max_depth_opts=seq(8, 14, 1),
          learn_rate_opts=c(0.01, 0.02),
          ## search a large space of row sampling rates per tree
          sample_rate = seq(0.4,1,0.2),

          ## search a large space of column sampling rates per split
          col_sample_rate_opts = seq(0.4,1,0.2),

          ## search a large space of column sampling rates per tree
          col_sample_rate_per_tree_opts = seq(0.4,1,0.2),

          ## search a large space of how column sampling per split should change as a function of the depth of the split
          col_sample_rate_change_per_level_opts = seq(0.9,1.1,0.1),

          ## search a large space of the number of min rows in a terminal node
          min_rows_opts = c(5, 10, 15, 20),

          ## search a large space of the number of bins for split-finding for continuous and integer columns
          nbins_opts = 2^seq(4,10,1),

          ## search a large space of the number of bins for split-finding for categorical columns
          nbins_cats_opts = 2^seq(4,10,1),

          ## search a few minimum required relative error improvement thresholds for a split to happen
          min_split_improvement_opts = c(1e-8, 1e-6, 1e-4),

          ## try all histogram types (QuantilesGlobal and RoundRobin are good for numeric columns with outliers)
          histogram_type_opts = c("UniformAdaptive","QuantilesGlobal","RoundRobin"),
	  strategy="RandomDiscrete"
    )

```


Here is the wrapper written to do the model fit.
```{r}

baseline.fit = function(hp, sc, independent.vars, grid_id, seed=SEED) {
        h2o.connect()
        m1 = paste0(names(hp), hp, collapse='\n')
        cat(paste0("Grid executing with params: \n", m1), file=stdout())
        h2o.grid.helper(
                        h2o.getFrame("XYTrain"),
                        h2o.getFrame("XYValidation"),
                        independentCols=independent.vars,
                        hyper_params = hp,
                        search_criteria = sc,
                        algorithm="gbm",
                        grid_id=grid_id,
                        seed = seed
                        )
        cat("Done", file=stdout())
}
```

Now lets train a few models. We train a few models a few times to compare learning rates

```{r training, cache=TRUE, eval=FALSE}
# TODO: Register a parallel backend using snow/doParallel
# Run the for each loop
# Get Cluster spec from h2o cluster.
# Register parallel backend for that cluster
# train the grids
instances = getExistingInstancesPublicDNS()
print(instances)
live_instances = instances
# The snow cluster is used if I want to execute the grid on multiple machines. A regular cluster is used if on local.

spec = clusterSpecFromInstances(live_instances)
clus = registerCluster(localOnly=FALSE, port=SNOW.PORT, instances.opts=spec, CORES.PER.MACHINE)
# foreach is quite expensive to just have iterators over all the hyper params so we have multiple grids and we iterate only over learning rate and depth
progress <- function() cat('.')
train.grid.wrapper = function(hp, sc) {
    foreach::foreach(
        nx = itertools::product(
            max_depth = hp$max_depth,
            learn_rate = hp$learn_rate
            ),
        grid_id = iter(1:NGrids),
        .packages=c("h2o", "dplyr", "magrittr", "futile.logger"),
        .export=c("baseline.fit", "h2o.grid.helper", "independent.vars"),
        .options.snow=list(progress=progress)) %dopar% {
                max_depth = nx$max_depth
                learn_rate = nx$learn_rate
                hp_t = hp
                hp_t$max_depth = max_depth
                hp_t$learn_rate = learn_rate

                return(baseline.fit(
                       hp=hp_t,
                       sc=sc,
                       grid_id=grid_id,
                       seed = as.integer(runif(1, 1e5, 1e6))))


        }
}

sc=search_criteria.gbm
hp=hyper_params.gbm
NGrids = length(hp$max_depth)*length(hp$learn_rate)
pergrid.models = Reduce('*', Map(function(x, init) length(x), hp) )/NGrids
flog.info(paste0( "parallel training ", NGrids , " grids with ", pergrid.models, " models each."))

```

As you can see training so many grids can take a long time. Instead we will use a few of them. It is better to spawn another R Console for the rest of the notebook.
```{r, eval=FALSE}
models = train.grid.wrapper(hp, search_criteria.gbm)
```

The grids above can take a long time to complete. But once a few 100 models have been trained we can just crack on with the analysis. Lets save the grids to disk and work offline with them.  You might want to source the code above and the constants to keep going. One issue with an h2o cluster is when interactive mode a failure on one node can cause models to dissapear.
```{r, eval=FALSE}
    h2o.connect(ip=CLUSTER.NODE.IP)
    models = h2o.gridSaver(1:14, results.dir=REMOTE.RESULTS.DIR)
```

The above command saves the models to a remote machine.
Copy the results to the local disk. Assuming you have access to the cluster you can execute the rsync command:
```{r, eval=FALSE}
cwd = getwd()
rsync.command=paste0('mkdir -p results/grid; rsync -Pavz -e "ssh -i $(ls ~/.ssh/sid-aws-key.pem)"  ubuntu@', CLUSTER.NODE.IP ,':', REMOTE.RESULTS.DIR,' ', LOCAL.RESULTS.DIR)
print(rsync.command)
system(rsync.command)
```

Instead of loading models from h2o we load them from disk.
```{r}
models = h2o.gridLoader(1:NGrids, results.dir=LOCAL.RESULTS.DIR)
all_summaries = model_summaries_from_loaded_models(models) %>% arrange(validation_rmse)
sorted.summaries = all_summaries %>% arrange(validation_rmse)
```

Lets look at the top 40 models in the grid. The folling is a sample of 106 models which is small as compared to the entire grid. A larger sample would be better. But there are still some useful observations that come out:
```{r, eval=FALSE}

nbins nbins_top_level nbins_cats learn_rate ntrees min_rows sample_rate col_sample_rate col_sample_rate_change_per_level col_sample_rate_per_tree min_split_improvement  histogram_type
  128            1024         64       0.02   1000       20         1.0             0.6                              1.0                      0.6                 1e-06      RoundRobin
   64            1024         64       0.02   1000       20         0.8             0.8                              1.0                      0.6                 1e-06      RoundRobin
 1024            1024         64       0.02   1000       20         0.6             0.8                              0.9                      0.4                 1e-06 QuantilesGlobal
   64            1024         64       0.01   1000       10         0.6             0.4                              1.1                      0.6                 1e-08      RoundRobin
   32            1024         16       0.01   1000       20         0.6             0.6                              1.0                      1.0                 1e-04      RoundRobin
  128            1024        128       0.02   1000       20         1.0             0.6                              0.9                      0.8                 1e-04      RoundRobin
  512            1024         32       0.02   1000       15         0.8             0.8                              1.1                      0.6                 1e-06 QuantilesGlobal
   32            1024         32       0.02   1000       20         0.8             0.6                              1.0                      0.8                 1e-04 QuantilesGlobal
   16            1024         32       0.02   1000       20         0.6             1.0                              1.0                      0.8                 1e-06      RoundRobin
   64            1024         32       0.02   1000       10         0.8             0.8                              1.1                      0.4                 1e-08 QuantilesGlobal
   64            1024         64       0.01   1000       20         0.8             0.4                              1.0                      1.0                 1e-06      RoundRobin
   64            1024         32       0.01   1000       20         0.8             0.4                              1.0                      0.8                 1e-04 UniformAdaptive
  256            1024         32       0.02   1000       20         1.0             0.4                              1.1                      0.6                 1e-06 QuantilesGlobal
   16            1024         64       0.01   1000       20         0.6             1.0                              1.0                      0.8                 1e-04 QuantilesGlobal
  512            1024        128       0.01   1000       15         0.6             0.4                              1.1                      0.8                 1e-06      RoundRobin
  256            1024         64       0.02   1000       15         0.4             1.0                              1.1                      0.4                 1e-06 QuantilesGlobal
  256            1024         16       0.02   1000       15         0.8             1.0                              1.0                      0.6                 1e-04 QuantilesGlobal
   64            1024         64       0.02   1000       10         0.8             1.0                              1.0                      0.6                 1e-04 UniformAdaptive
  256            1024         32       0.01   1000       10         0.6             0.8                              1.0                      0.8                 1e-06      RoundRobin
  128            1024         64       0.02   1000        5         1.0             0.6                              1.1                      0.4                 1e-04 UniformAdaptive
   number_of_trees number_of_internal_trees min_depth max_depth mean_depth min_leaves max_leaves mean_leaves validation_rmse train_rmse
1              200                      200        12        12   12.00000         20        348   151.05000       0.1576870  0.1513393
2              205                      205        12        12   12.00000         23        322   126.92683       0.1577111  0.1525869
3              189                      189        14        14   14.00000         58        531   216.59259       0.1577426  0.1531752
4              267                      267        14        14   14.00000         58        583   252.38202       0.1577857  0.1522151
5              238                      238        13        13   13.00000         30        331   132.21008       0.1578537  0.1567330
6              103                      103        14        14   14.00000         39        481   250.73787       0.1578568  0.1523961
7              146                      146        12        12   12.00000         32        344   185.65753       0.1578600  0.1515632
8              144                      144        10        10   10.00000         39        194    96.32639       0.1578612  0.1554228
9               97                       97        14        14   14.00000         37        355   154.49484       0.1578645  0.1556081
10             152                      152        11        11   11.00000         39        387   174.83553       0.1578730  0.1513839
11             275                      275        11        11   11.00000         17        245   110.35636       0.1578783  0.1563598
12             258                      258        13        13   13.00000         32        490   164.75581       0.1578937  0.1559698
13             135                      135        12        12   12.00000         48        433   198.76297       0.1579010  0.1520331
14             225                      225        12        12   12.00000         41        304   144.99556       0.1579131  0.1558366
15             241                      241        14        14   14.00000         48        520   195.71785       0.1579200  0.1525593
16             158                      158        10        10   10.00000         30        197    91.31013       0.1579432  0.1563515
17             113                      113         9        12   11.97345         15        402   191.47787       0.1579473  0.1531330
18             120                      120        11        11   11.00000         29        262   119.90833       0.1579483  0.1539735
19             211                      211        13        13   13.00000         34        363   164.52133       0.1579587  0.1549085
20             182                      182         9         9    9.00000         18        242    90.03297       0.1579593  0.1524554
```
##### Tuning lessons
- nbins_cats of 16-64 range is enough. The top 20 values it leans more towards 16, 32 than 64
- the depth of the top trees is between 12-14.
- sample rate is happy being between 0.6-0.8. Although there are two values in there that are 1 top 20
- col_sample_rate for top models is between 0.4-0.8 for the most part
- The QuantilesGlobal and RoundRobin domniate the top 20. We can drop Uniform Adaptive

So we can narrow the tuning grid with the following changes

```{r}

hp=hyper_params.gbm
hp$col_sample_rate = seq(0.4, 0.8, 0.1)
hp$max_depth= seq(10, 14, 1)
hp$sample_rate = seq(0.4, 0.8, 0.1)
hp$nbins_cats= 2^seq(4,6,1)
hp$histogram_type= c("QuantilesGlobal", "RoundRobin")
NGrids = length(hp$max_depth)*length(hp$learn_rate)
pergrid.models = Reduce('*', Map(function(x, init) length(x), hp) )/NGrids
flog.info(paste0( "After narrowing grid search, parallel training with ", NGrids , " grids with ", pergrid.models, " models each."))
```

Before we narrow our grid search to improve our confidence in the chosen parameters, lets do some baseline predictions after training the top 3 models. We will use training + validation data to Cross validate the top K models and look at:
1. Model with the small cross-validated RMSE
2. Deviation of RMSE from the best model over the 5 folds.

```{r eval=FALSE}

retrainWrapper.cv <- function(gbm, train.df, validation.df) {
    return(do.call(h2o.gbm,
        ## update pasameters in place
        {
          p <- gbm@parameters
          p$model_id = NULL          ## do not overwrite the original grid model
          p$training_frame = train.df      ## use the full dataset
          p$validation_frame = validation.df  ## no validation frame
          p$nfolds = 5               ## cross-validation
          p
     }))
}
# The local cluster for doing the parallel computation
cls = registerCluster(max.cores.per.machine = 6)

topk.ids = sorted.summaries %>% pull(model_id) %>% as.character
# Create a list of crossvalidated GBM model, indexed by the topk model id's. 20 top models are selected
cvgbms = foreach::foreach (
       i = iter(1:20), .packages=("h2o"),
       .final = function(x) setNames(x, topk.ids[1:20])) %dopar% {
    h2o.connect()
    gbm <- h2o.getModel(topk.ids[[i]])
    cvmodel = retrainWrapper.cv(
          gbm,
          train.df = h2o.rbind(XYTrain.h2o, XYValidation.h2o),
          validation.df = XYTest.h2o)
    # We note the modelid of the GBM's so as to associate them with the odel from whcih they were created
    cvmodel@model$origin.model = topk.ids[[i]]
    return(cvmodel)
}
```

Save these models for offline work.
```{r gbms, cache=TRUE}
    gbm.results.dir = file.path(LOCAL.RESULTS.DIR, "cv_models")
    dir.create(gbm.results.dir)
    i = 1
    for(m in cvgbms) {
        if(!is.null(m)) {
            fp = file.path(gbm.results.dir, m@model$origin.model)
            # These models were obtained after a some effort, so this guards against overwriting/polluting them by accident
            #stopifnot(
            #      !file.exists(file.path(
            #                     gbm.results.dir, topk.ids[[i]]))) 
            dir.create(fp)
            #h2o.saveModel(m, path=fp) # Don't try and save with saveRDS, it does not work
            h2o.download_pojo(m, path=fp) # Don't try and save with saveRDS, it does not work
            cat(".")
        }
        i = i + 1
    }

```

Optionally load the models if you are resuming work.
```{r saveTunedGBMs, eval=FALSE}
# Since the models are loaded from disk. the order they are in is not the same as the topk.ids. So we need to take care of that here
gbm.results.dir = file.path(LOCAL.RESULTS.DIR, "cv_models")
if(!dir.exists(gbm.results.dir)) {
    flog.warn(paste0("Dir ", gbm.results.dir, " does not exist"))
    stopifnot(FALSE)
}

cvgbms = list()
for(m.id_file in list.files(gbm.results.dir, recursive=TRUE)) {

    m.id_file.path = file.path(gbm.results.dir, m.id_file)
    if(endsWith(m.id_file.path, ".java") || endsWith(m.id_file.path, ".jar")) { # Saved POJOs
        flog.info(paste("Not loading ", m.id_file.path))
        next    
    }
    print(".")
    # Find the slot of the loaded model in the current files
    flog.debug(paste0("Loading ", m.id_file.path))
    m = h2o.loadModel(m.id_file.path)
    cvgbms[[m@model$origin.model]] = m
    print('..')
}

```

So how do the cross validated models score on the hold out data set?

```{r, tidy = TRUE, message = FALSE, warning = FALSE, error = FALSE, eval=FALSE}

grid.scores.topk =  Reduce(rbind,
                           Map(function(m) c(
                                             h2o.rmse(h2o.performance(m, train=TRUE)),
                                             h2o.rmse(h2o.performance(m, valid=TRUE)),
                                             m@model_id),
                               Map(h2o.getModel,topk.ids[1:20])))

colnames(grid.scores.topk) = c("training_rmse", "validation_rmse", "model_id")

grid.scores.topk %<>% as.data.frame(stringsAsFactors=FALSE) %>% 
    dplyr::mutate(training_rmse = as.numeric(training_rmse), validation_rmse=as.numeric(validation_rmse)

xval.scores.topk = Reduce(rbind,
    Map(function(i) {
            m = cvgbms[[topk.ids[i]]]
            if(is.null(m)) {
                flog.warn(paste0("CV slot for model #",i, ", id: ", topk.ids[[i]]," was null."))
                return(rep(NA,4))
            } else {
                return(c(h2o.rmse(h2o.performance(m, train=TRUE)),
                          h2o.rmse(h2o.performance(m, xval=TRUE)),
                          as.numeric(m@model$cross_validation_metrics_summary['rmse', ]$sd),
                          h2o.rmse(h2o.performance(m, valid=TRUE)),
                          topk.ids[[i]]))
            }},
         1:20))

# Following metrics are plotted:
 # training_rmse: The model's error on the training data set
 # validation_rmse: The model's error on the validation data set
 # training_cv_rmse: The training error on cross validated data,
 # five_fold_cv_rmse": The combined rmse on holdout set of the crossvalidated data
 # five_fold_cv_rmse_sd": the sd of the K fold CV RMSEs
 # validation_cv_rmse: The error of the cross validated model on the hold out data
colnames(xval.scores.topk) = c("training_cv_rmse", "five_fold_cv_rmse",
                               "five_fold_cv_rmse_sd", "validation_cv_rmse",
                               "model_id")
combined.scores = cbind(grid.scores.topk, xval.scores.topk) %>% data.frame
rownames(combined.scores) = 1:20
print(combined.scores)
combined.scores %<>% na.omit %>% mutate(x=rownames(.))
ggplot(combined.scores) +
    geom_line(aes(x=x, y = validation_cv_rmse, color='validation_cv_rmse', group=1)) +
    geom_line(aes(x=x, y = validation_rmse, color='validation_rmse', group=1)) +
    geom_line(aes(x=x, y = five_fold_cv_rmse, color='five_fold_cv_rmse', group=1)) +
    geom_errorbar(aes(x=x,
                  ymin = five_fold_cv_rmse + five_fold_cv_rmse_sd,
                  ymax = five_fold_cv_rmse - five_fold_cv_rmse_sd,
                  color='five_fold_cv_rmse', group=1), alpha=0.5, size=0.5)


```
And here are the end results

```{r eval=FALSE}
> combined.scores %>% dplyr::arrange(validation_rmse) %>% head(5)
  training_rmse validation_rmse training_cv_rmse five_fold_cv_rmse five_fold_cv_rmse_sd validation_cv_rmse x
1     0.1524634       0.1575912        0.1546496         0.1607562          0.003245448          0.1550688 1
2     0.1536710       0.1576838        0.1541283         0.1605204          0.002876918          0.1547557 2
3     0.1510411       0.1576987        0.1531384         0.1606293          0.003212974          0.1547801 3
4     0.1541381       0.1577222        0.1543080         0.1605372          0.004817895          0.1547763 4
5     0.1542500       0.1577269        0.1553290         0.1605997          0.002967700          0.1549573 5
> combined.scores %>% dplyr::arrange(validation_cv_rmse) %>% head(5)
  training_rmse validation_rmse training_cv_rmse five_fold_cv_rmse five_fold_cv_rmse_sd validation_cv_rmse  x
1     0.1516028       0.1577624        0.1523159         0.1607009          0.003225586          0.1547194 11
2     0.1547744       0.1577506        0.1537014         0.1607162          0.003218554          0.1547330  8
3     0.1538070       0.1577549        0.1548555         0.1605744          0.005776218          0.1547443  9
4     0.1525488       0.1578152        0.1526349         0.1606426          0.002971843          0.1547493 19
5     0.1536710       0.1576838        0.1541283         0.1605204          0.002876918          0.1547557  2
> combined.scores %>% dplyr::arrange(five_fold_cv_rmse_sd) %>% head(5)
  training_rmse validation_rmse training_cv_rmse five_fold_cv_rmse five_fold_cv_rmse_sd validation_cv_rmse  x
1     0.1536710       0.1576838        0.1541283         0.1605204          0.002876918          0.1547557  2
2     0.1557194       0.1577673        0.1566060         0.1607139          0.002946662          0.1549478 12
3     0.1542500       0.1577269        0.1553290         0.1605997          0.002967700          0.1549573  5
4     0.1525488       0.1578152        0.1526349         0.1606426          0.002971843          0.1547493 19
5     0.1551364       0.1577394        0.1541773         0.1605682          0.003007525          0.1549236  6
```

##### Takeaways
- The crossvalidation improves the performance on the hold out data set(compare validation_rmse and validation_cv_rmse)
- The cross-validation rmse are pretty high(ish). Its possible that because of the task of modelling the log error the task the amount of noise in the data set is high. Regardless of the cause of the high variance, [this](https://github.com/h2oai/h2o-3/blob/master/h2o-docs/src/product/cross-validation.rst) link indicates that becasue of it, its hard to fully "trust" the top models.
- Off The top models which did well(see validation_rmse) on holdout data set, only model 2 appeared in the top 5.

So along with the model that did the best in cross validation, I made the following additional submissions:
    - Model 12.
    - Model 12 but after retraining on the full data set.
    - Ensemble of top 5 CV models.
    - Ensemble of top 5 CV models but after retraining on the entire data set.


Lets look at the predictions for the top models
```{r, eval=FALSE}

models.idxs = c(8,9,1) # The indexes of the top models 
# The dates needed for the levels
predict.dates = c("2016-10-01", "2016-11-01", "2016-12-01")
predict.dates.numeric.codes = which(levels(t.dates) %in% predict.dates)
parcel_ids = as.integer(pr.baseline$id_parcel)

cv.predictions <- foreach(
            nx = itertools::product(
                 #model_id = iter(Map(function(m) m@model_id, cvgbms[models.idxs])),
                 i = iter(1:length(models.idxs))
                 #X=itertools::isplitRows(pr.baseline, chunks=10)
            ),
            .packages=c("magrittr", "h2o", "dplyr")) %do% {
                cat(paste0("Prediction for Model: ", cvgbm[[models.idxs[nx$i]]]@model_id, "\n"), stdout())
                #h2o.connect() 
                #gbm.model = h2o.getModel(nx$model_id)
                print("..")
                if(DATE.FEATURE) {
                    Map(function(date.code) {
                        X = as.h2o(pr.baseline %>% dplyr::mutate(date=date.code), destination_frame="XPredict")
                        #ds = h2o.predict(gbm.model, X) %>% as.data.frame
                        ds = h2o.predict(h2o.get(cvgbm[[models.idxs[i]]], X) %>% as.data.frame
                        h2o.rm("XPredict")
                        print(".")
                        ds = cbind(data.frame(parcelid=parcel_ids), ds, ds)
                        print(paste(dim(ds)))
                        colnames(ds) = c("parcelid", "201610", "201611" ,"201612", "201710", "201711", "201712")
                        print("..")
                    }, predict.dates.numeric.codes)
                } else {
                    predictions = h2o.predict(h2o.get(cvgbm[[models.idx]], X), 
                                              as.h2o(pr.baseline, destination_frame="XPredict")) %>% 
                    as.data.frame

                    ds = data.frame(parcelid=parcel_ids, 
                                    "201610"=predictions, 
                                    "201611"=predictions, 
                                    "201612"=predictions,
                                    "201710"=predictions,
                                    "201711"=predictions,
                                    "201712"=predictions) 
                    colnames(ds) = c("parcelid", "201610", "201611" ,"201612", "201710", "201711", "201712")
                }
                writePredictions(predictions = ds,
                                 filename.suffix=paste0( 
                                        "cross_validated_", models.idxs[i], "_",
                                        cvgbms[[topk.ids[[models.idxs[i]]]]]@model_id))
                rm(ds)
                cat(paste0("Size: ", nrow(date.predictions), "\n"))
                date.predictions
}



```


