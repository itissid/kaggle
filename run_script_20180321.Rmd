---
title: Tuning the GBM learning rate
author: "Sidharth Gupta"
date: \today
output:
   html_document:
    css: css/code.css
    highlight: default
    citation_package:
    keep_tex: false
    fig_caption: true
    latex_engine: pdflatex
runtime: shiny
fontsize: 11pt
geometry: margin=1ine
header-includes:
- \usepackage{indentfirst}
- \usepackage{graphicx}
- \usepackage{geometry}
- \usepackage{subfigure}
- \usepackage{amsmath}
- \usepackage{listings}
- \usepackage{tikz}
- \usetikzlibrary{matrix}
---

```{r setup, tidy = TRUE, message = FALSE, warning = FALSE, error = FALSE, results='hide'}
library(dplyr)
library(magrittr)
library(gridExtra)
library(ggplot2)
library(h2o)
library(gridExtra)
library(futile.logger)
source("h2o/source-for-rmd.R")
h2o.init()
SEED=123456
```

Tuning the learning rate. First lets prepare some data

```{r data, cache=TRUE, message=FALSE, error=FALSE, warning=FALSE, echo=FALSE, results='hide'}
# The function for reading the data does a few things by default. It makes categorical  variables where ever necessar, and log transforms the large variables like for taxes etc. See documentation of that function.
list[tr.baseline, pr.baseline, vtreatIdentityFn, tplan.NULL] =  prepareDataWrapper.h2o.gbm.baseline()

independent.vars = colnames(tr.baseline %>% select(-logerror))

list[XTrain, YTrain, XHoldout, YHoldout] = splitTrainingWrapper(tr.baseline, split_percent=0.90, YName="logerror")

XYTrain.h2o = as.h2o(x = cbind(XTrain, logerror=YTrain), destination_frame="XTrain.h2o")
XYTest.h2o = as.h2o(x = cbind(XHoldout, logerror=YHoldout), destination_frame="XTest.h2o")

XPredict = as.h2o(pr.baseline, destination_frame="XPredict")
```

Now lets create a grid that only varies the learning rate only. We choose a large number of trees. But becasue we have early stopping we should be ok.

```{r hyperparams_search_crit}
hyper_params_search_crit.learnrate.tune = function(
            ntrees_opts = 10000, # early stopping will stop the earlier.
            max_depth_opts = 10,  # Start with 6 for now
            min_rows_opts = 5,   # How many leaf nodes to average over, doing the log of # rows would be ok
            learn_rate_opts = 0.01,
            learn_rate_annealing = 1,
            sample_rate_opts = 0.9,
            col_sample_rate_opts = 0.9,
            col_sample_rate_per_tree_opts = 0.9,
            strategy="RandomDiscrete") {

    list[search_criteria, hyper_params] =  hyper_params_search_crit(
            ntrees_opts = ntrees_opts,
            max_depth_opts = max_depth_opts,
            min_rows_opts = min_rows_opts,
            learn_rate_opts = learn_rate_opts, # from the previous
            learn_rate_annealing = learn_rate_annealing,
            sample_rate_opts = sample_rate_opts,
            col_sample_rate_opts = col_sample_rate_opts,
            col_sample_rate_per_tree_opts = col_sample_rate_per_tree_opts,
            strategy=strategy)
    return(list(search_criteria, hyper_params))
}

```

Here is the wrapper written to do the model fit.
```{r}

baseline.fit = function(hp, sc, grid_id = "learning_rate_search", seed=SEED, ...) {
        h2o.grid.helper(
                        XYTrain.h2o,
                        XYTest.h2o,
                        independentCols=independent.vars,
                        hyper_params = hp,
                        search_criteria = sc,
                        algorithm="gbm",
                        grid_id=grid_id,
                        seed = seed,
                        ...
                        )
}
```

Now lets train a GBM on a grid and cross validate the learn_rate parameter. I used 5-fold cross validation.

```{r training, cache=TRUE}
set.seed(123456)
target_learning_rates = seq(0.01, 0.05, 0.01)
list[search_criteria.gbm.learnrate, hyper_params.gbm.learnrate] =
    hyper_params_search_crit.learnrate.tune(learn_rate_opts=target_learning_rates)
fit.grid = baseline.fit(
           hp=hyper_params.gbm.learnrate,
           sc=search_criteria.gbm.learnrate, nfolds=5)
```

Look at the top K learning rates by the error:
```{r}
getRMSEScoreHistory = function (models) {
    # Given a list of model ids get a data frame with its RMSE score history
    score.history = data.frame()
    # Learning rate of 0.03 seems to be ok.
    for (i in 1:length(models)) {
        lr = models[[i]]@parameters$learn_rate
        v_rmse= h2o.scoreHistory(models[[i]]) %>%
            data.frame %>%
            pull(validation_rmse)
        t_rmse= h2o.scoreHistory(models[[i]]) %>%
            data.frame %>%
            pull(training_rmse)
        score.history = rbind(score.history,
                              data.frame(
                                         ntrees = 1:length(v_rmse),
                                         validation_rmse = v_rmse,
                                         training_rmse = t_rmse,
                                         id = i,
                                         lr = lr,
                                         id = paste("model_", i, "_lr_", lr, sep='')
                                         )
                              )
    }
    score.history
}
```

Lets see how the actual learning rates look like for each of the crossvalidated models in the grid. This will give me an idea of how fast or slow the learning progresses.
```{r plots, fig.width=10, fig.height=10}
    models = sapply(FUN=function(m) h2o.getModel(m), fit.grid@model_ids)
    score.history.combined = getRMSEScoreHistory(models)
    # I initially tried working with a lits of plots instead of plotting the data directly. Had trouble sizing the plots properly. So backed off and instead used a combined version of score history with vanialla ggplot.
    min_y_lim = score.history.combined %>% pull(rmse) %>% min
    max_y_lim = score.history.combined %>% pull(rmse) %>% max

    ggplot(score.history.combined) +
            facet_wrap(~ lr) + 
            geom_line(aes(y=validation_rmse, x=ntrees, color=as.factor(id))) +
            ylim(c(min_y_lim, max_y_lim)) +
            theme(
              legend.position="NULL",
              strip.background = element_blank()
            )

```

It seems clear that 13/15 of the curves the learning curves corresponding to 0.01-0.02 end up lower without the validation error overshooting.
