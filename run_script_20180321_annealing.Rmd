---
title: Tuning the GBM learning rate with annealing
author: "Sidharth Gupta"
date: \today
output:
   html_document:
    css: css/code.css
    highlight: default
    toc: true
    toc_depth: 6
    citation_package:
    keep_tex: false
    fig_caption: true
    latex_engine: pdflatex
runtime: shiny
fontsize: 11pt
geometry: margin=1ine
header-includes:
- \usepackage{indentfirst}
- \usepackage{graphicx}
- \usepackage{geometry}
- \usepackage{subfigure}
- \usepackage{amsmath}
- \usepackage{listings}
- \usepackage{tikz}
- \usetikzlibrary{matrix}
---

```{r setup, tidy = TRUE, message = FALSE, warning = FALSE, error = FALSE, results='hide'}
library(dplyr)
library(magrittr)
library(gridExtra)
library(ggplot2)
library(h2o)
library(gridExtra)
library(futile.logger)
source("h2o/source-for-rmd.R")
SEED=123456
h2o.init()
```

```{r, echo=FALSE}
# KNITR CONTROL OPTIONS:
LOAD.DATA=FALSE # Once you run the NB and loaded the data you may want to make this false
LOAD.DISK.MODELS=TRUE # Once you run the NB the models will be loaded to disk, you might want to kill this option then
```

A few of the sections that define the data and the functions are same as the last notebook on learning rate tuning and are not repeated
```{r data, cache=TRUE, message=FALSE, error=FALSE, warning=FALSE, echo=FALSE, results='hide', eval=LOAD.DATA==TRUE}
# The function for reading the data does a few things by default. It makes categorical  variables where ever necessar, and log transforms the large variables like for taxes etc. See documentation of that function. 
list[tr.baseline, pr.baseline, vtreatIdentityFn, tplan.NULL] =  prepareDataWrapper.h2o.gbm.baseline()

independent.vars = colnames(tr.baseline %>% select(-logerror))


list[XYTrain.h2o, XYTest.h2o] = h2o.splitFrame(as.h2o(tr.baseline), ratios = 0.85, destination_frames=c("XYTrain.h2o", "XYTest.h2o"), seed=SEED)
```

## Annealing added to the tree parameters
Change the hyper parameters to contain the annealing parameter.
```{r training.annealed, cache=TRUE, results="hide"}
target_learning_rates = seq(0.01, 0.05, 0.01)
# Just change the annealing rate for the hyper parameters
list[search_criteria.gbm.learnrate, hyper_params.gbm.learnrate.annealed] = hyper_params_search_crit.learnrate.tune(
                      learn_rate_opts=target_learning_rates, 
                      learn_rate_annealing=0.995)
# The search parameters and hyper params to the GBM are
print(search_criteria.gbm.learnrate %>% data.frame)
print(hyper_params.gbm.learnrate.annealed %>% data.frame)
```
The code is exactly the same as before. I will just display the graph of the cross validated learning rate repeated 10 times.

```{r training, cache=TRUE, eval=FALSE, results="hide", echo=FALSE}
# Randomize and search for the best learning rate.
fit.annealed = baseline.fit(
       hp=hyper_params.gbm.learnrate.annealed,
       sc=search_criteria.gbm.learnrate,
       grid_id = paste("learning_rate_search_annealed"),
       seed = SEED, 
       nfolds=5)

```

## Analysis
Lets see what are the top learning rates:
```{r}
kable(fit.annealed@summary_table %>% data.frame %>% select(learn_rate, residual_deviance) %>% arrange(residual_deviance))
```

### Plotting the individual learning rate.
```{r, eval=FALSE, echo=FALSE, results='hide'}
LOCAL.RESULTS.DIR=paste0(getwd(), "/results/learn_rate_2018_15_04_gbm")
if(!dir.exists(LOCAL.RESULTS.DIR)) 
    dir.create(LOCAL.RESULTS.DIR)
h2o.gridSaver(c("learning_rate_search"), results.dir=LOCAL.RESULTS.DIR)
```


```{r, eval=FALSE, echo=FALSE, results='hide'}
NGrids = 10
cls = registerCluster(max.cores.per.machine = 6)

fit.anneaeled.bootstrapped = foreach(x = 1:NGrids, .packages= c("h2o")) %dopar% {
              h2o.connect()
              baseline.fit(
                       hp=hyper_params.gbm.learnrate.annealed,
                       sc=search_criteria.gbm.learnrate,
                       grid_id = paste("learning_rate_search_annealed_", x, sep=''),
                       seed = x)
}
```

```{r, results='hide', echo=F, eval=F}

LOCAL.RESULTS.DIR=paste0(getwd(), "/results/learn_rate_annealed_2018_15_04_gbm.bootstrapped")
if(!dir.exists(LOCAL.RESULTS.DIR)) 
    dir.create(LOCAL.RESULTS.DIR)
h2o.gridSaver(Map(function(x) paste0("learning_rate_search_annealed_", x) , 1:NGrids), results.dir=LOCAL.RESULTS.DIR)
```

```{r, results='hide', echo=FALSE, eval=LOAD.DISK.MODELS==TRUE}
LOCAL.RESULTS.DIR=paste0(getwd(), "/results/learn_rate_annealed_2018_15_04_gbm.bootstrapped")
models = h2o.gridLoader(Map(function(x) paste0("learning_rate_search_annealed_", x) , 1:NGrids), results.dir=LOCAL.RESULTS.DIR)
```

```{r plots.grid, echo=FALSE, fig.width=10, fig.height=10}
# A list of lists of all the trained models
models.lst = sapply(X = 1:NGrids,
                    FUN=function(x) {
                        grid.id = h2o.getGrid(paste("learning_rate_search_annealed_", x, sep=''))
                        sapply(X=grid.id@model_ids, FUN=h2o.getModel)
                    })
score.history.lst = sapply(X = 1:NGrids,
                           function(x) getRMSEScoreHistory(models.lst[, x]))

score.history.combined = Reduce(function(x, init) {
               rbind(x, init)
          }, Map(function(i) {
                score.history.lst[, i] %>%
                    data.frame %>%
                    mutate(grid_id=i)
            }, 1:NGrids)
)

score.history.combined %>% group_by(grid_id, lr) %>% 
    dplyr::filter(ntrees==max(ntrees)) %>%
    data.frame %>% group_by(grid_id) %>% 
    dplyr::filter(rmse ==min(rmse)) %>%
    arrange(rmse) %>% kable

# I initially tried working with a lits of plots instead of plotting the data directly. Had trouble sizing the plots properly. So backed off and instead used a combined version of score history with vanialla ggplot.
min_y_lim = score.history.combined %>% pull(rmse) %>% min
max_y_lim = score.history.combined %>% pull(rmse) %>% max

ggplot(score.history.combined) +
    facet_wrap(~ grid_id, ncol=2) +
        geom_line(aes(y=rmse, x=ntrees, color=as.factor(lr)),
                  position=position_dodge(1)) +
        ylim(c(min_y_lim, max_y_lim)) +
        theme(
          legend.position="bottom",
          strip.background = element_blank(),
          strip.text.x = element_blank()
        )

```

## Conclusion
The RMSE with/without annealing does not change. The learnig rates of 0.1-0.2 perform much better with annealing.
