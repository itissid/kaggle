---
title: Tuning the GBM learning rate with annealing
author: "Sidharth Gupta"
date: \today
output:
   html_document:
    css: css/code.css
    highlight: default
    citation_package:
    keep_tex: false
    fig_caption: true
    latex_engine: pdflatex
runtime: shiny
fontsize: 11pt
geometry: margin=1ine
header-includes:
- \usepackage{indentfirst}
- \usepackage{graphicx}
- \usepackage{geometry}
- \usepackage{subfigure}
- \usepackage{amsmath}
- \usepackage{listings}
- \usepackage{tikz}
- \usetikzlibrary{matrix}
---

```{r setup, tidy = TRUE, message = FALSE, warning = FALSE, error = FALSE}
library(dplyr)
library(magrittr)
library(gridExtra)
library(ggplot2)
library(h2o)
library(gridExtra)
library(futile.logger)
source("h2o/source-for-rmd.R")
```


```{r data, cache=TRUE, message=FALSE, error=FALSE, warning=FALSE, echo=TRUE, results='hide'}
# The function for reading the data does a few things by default. It makes categorical  variables where ever necessar, and log transforms the large variables like for taxes etc. See documentation of that function. 
list[tr.baseline, pr.baseline, vtreatIdentityFn, tplan.NULL] =  prepareDataWrapper.h2o.gbm.baseline()

independent.vars = colnames(tr.baseline %>% select(-logerror))

list[XTrain, YTrain, XHoldout, YHoldout] = splitTrainingWrapper(tr.baseline, split_percent=0.90, YName="logerror") 

XYTrain.h2o = as.h2o(x = cbind(XTrain, logerror=YTrain), destination_frame="XTrain.h2o")
XYTest.h2o = as.h2o(x = cbind(XHoldout, logerror=YHoldout), destination_frame="XTest.h2o")
```

Change the hyper parameters to contain the annealing parameter.
```{r training.annealed, cache=TRUE, results="hide"}
set.seed(123456) 
target_learning_rates = seq(0.01, 0.05, 0.01)
# Just change the annealing rate for the hyper parameters
list[search_criteria.gbm.learnrate, hyper_params.gbm.learnrate] = hyper_params_search_crit.learnrate.tune(
                                                                      learn_rate_opts=target_learning_rates, 
                                                                      learn_rate_annealing=0.995)
# The search parameters and hyper params to the GBM are
print(search_criteria.gbm.learnrate %>% data.frame)
print(hyper_params.gbm.learnrate %>% data.frame)
fits = sapply(X = 1:NGrids, 
              FUN=function(x) baseline.fit(
                       hp=hyper_params.gbm.learnrate, 
                       sc=search_criteria.gbm.learnrate, 
                       grid_id = paste("learning_rate_search_annealed", x, sep=''), 
                       seed = as.integer(runif(1, 1e5, 1e6))))
```

Grab the models asame as last time
```{r modellist, cache=TRUE}
# A list of lists of all the trained models
models.lst = sapply(X=1:NGrids,
                    FUN=function(x) {
                        grid.ids = h2o.getGrid(paste("learning_rate_search_annealed", x, sep=''))
                        sapply(X=grid.ids@model_ids, FUN=h2o.getModel)
                    })

```

Grab all the model's score history same as last time
```{r modellist.annealed, results='hide', cache=TRUE}
models.lst = sapply(X=1:NGrids,
                    FUN=function(x) {
                        the.grid = h2o.getGrid(paste("learning_rate_search_annealed", x, sep=''))
                        if(length(the.grid@failed_params) == 0) {
                            return(sapply(X=the.grid@model_ids, FUN=h2o.getModel))
                       } else {
                           flog.warn(paste("Model with ID:", the.grid@grid_id, " failed"))
                           return(sapply(X=the.grid@model_ids, function(x) NA))
                       }
                    })
```

Combine all the histories into one frame.
```{r plots.annealed, results='markup'}
# For each model list in models.lst extract the histories
score.history.lst = sapply(X = 1:NGrids, 
                           function(x) getRMSEScoreHistory(models.lst[, x])) 

score.history.combined =
    Reduce(function(x, init) {
               rbind(x, init)
          }, Map(function(i) {
                score.history.lst[, i] %>%
                    data.frame %>%
                    mutate(grid_id=i)
            }, 1:NGrids)
    )
```

Plot em!
```{r plots, fig.width=10, fig.height=10}
min_y_lim = score.history.combined %>% pull(rmse) %>% min
max_y_lim = score.history.combined %>% pull(rmse) %>% max

ggplot(score.history.combined) +
    facet_wrap(~ grid_id, ncol=2) +
        geom_line(aes(y=rmse, x=ntrees, color=as.factor(lr)),
                  position=position_dodge(1)) +
        ylim(c(min_y_lim, max_y_lim)) +
        theme(
          legend.position="bottom",
          strip.background = element_blank(),
          strip.text.x = element_blank()
        )
```
Annealing results in more consistent RMSE scores in the 0.1-0.2 range

```{r}
score.history.combined %>% group_by(grid_id, lr) %>% 
    dplyr::filter(ntrees==max(ntrees)) %>%
    data.frame %>% group_by(grid_id) %>% 
    dplyr::filter(rmse ==min(rmse))

```

```{r}
score.history.combined %>% 
    group_by(grid_id) %>% 
    dplyr::filter(rmse == min(rmse)) %>% 
    data.frame %>% 
    arrange(rmse)
```

The RMSE due to annealing is higher than without(see previous notebook) which is to be expected.
But annealing reveals that the learning rates between 0.1-0.2 (ultimately to be choosen by grid search) are the right choice, even if we *don't* end up using annealing in the final fit.
